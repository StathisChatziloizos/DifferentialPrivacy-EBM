{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_adult_data():\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "    df = pd.read_csv(url, header=None)\n",
    "    df.columns = [\n",
    "        \"Age\", \"WorkClass\", \"fnlwgt\", \"Education\", \"EducationNum\",\n",
    "        \"MaritalStatus\", \"Occupation\", \"Relationship\", \"Race\", \"Gender\",\n",
    "        \"CapitalGain\", \"CapitalLoss\", \"HoursPerWeek\", \"NativeCountry\", \"Income\"\n",
    "    ]\n",
    "    # Convert numeric Income to binary: 0 if <=50K, 1 if >50K\n",
    "    df['Income'] = df['Income'].apply(lambda x: 0 if x.strip() == \"<=50K\" else 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_telco_churn_data():\n",
    "    # https://www.kaggle.com/blastchar/telco-customer-churn/downloads/WA_Fn-UseC_-Telco-Customer-Churn.csv/1\n",
    "    # df = pd.read_csv(r'C:\\develop\\data\\telco-customer-churn\\WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "    df = pd.read_csv(f\"datasets/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "    train_cols = df.columns[1:-1] # First column is an ID\n",
    "    label = df.columns[-1]\n",
    "    X_df = df[train_cols]\n",
    "    y_df = df[label] # 'Yes, No'\n",
    "    y_df = y_df.apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "    dataset = {\n",
    "        'problem': 'classification',\n",
    "        'full': {\n",
    "            'X': X_df,\n",
    "            'y': y_df,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_credit_data():\n",
    "    # https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "    # df = pd.read_csv(r'C:\\develop\\data\\creditcardfraud\\creditcard.csv')\n",
    "    df = pd.read_csv(f\"datasets/creditcard.csv\")\n",
    "    train_cols = df.columns[0:-1]\n",
    "    label = df.columns[-1]\n",
    "    X_df = df[train_cols]\n",
    "    y_df = df[label]\n",
    "    dataset = {\n",
    "        'problem': 'classification',\n",
    "        'full': {\n",
    "            'X': X_df,\n",
    "            'y': y_df,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom DP-EBM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dp_gaussian_noise(scale, size=None):\n",
    "    \"\"\"Generate Gaussian noise with standard deviation 'scale'.\"\"\"\n",
    "    return np.random.normal(0, scale, size=size)\n",
    "\n",
    "def bin_feature(x, n_bins=32):\n",
    "    \"\"\"\n",
    "    For a 1D numpy array x, if numeric then bin using quantiles;\n",
    "    otherwise (categorical) map each unique value to an index.\n",
    "    \n",
    "    Returns:\n",
    "      bins: array of bin indices (integers)\n",
    "      edges: if numeric, the bin edge values; if categorical, the sorted unique values.\n",
    "    \"\"\"\n",
    "    # Check if numeric (or convertible to float)\n",
    "    try:\n",
    "        # If all elements can be cast to float, we assume numeric.\n",
    "        x_float = x.astype(float)\n",
    "        quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "        edges = np.quantile(x_float, quantiles)\n",
    "        # Ensure edges are unique (if data is concentrated, there may be fewer bins)\n",
    "        edges = np.unique(edges)\n",
    "        bins = np.digitize(x_float, edges, right=False) - 1\n",
    "        bins = np.clip(bins, 0, len(edges) - 2)\n",
    "        return bins, edges\n",
    "    except (ValueError, TypeError):\n",
    "        # Otherwise treat as categorical.\n",
    "        uniques = np.unique(x)\n",
    "        # Sort the unique values (this defines an order)\n",
    "        uniques = np.sort(uniques)\n",
    "        # Map each category to its index in the sorted list.\n",
    "        mapping = {val: i for i, val in enumerate(uniques)}\n",
    "        bins = np.array([mapping[val] for val in x])\n",
    "        return bins, uniques\n",
    "\n",
    "# DP-EBM training\n",
    "def dp_ebm_train(X, y, n_bins=32, n_epochs=300, learning_rate=0.01, epsilon=1.0, delta=1e-5):\n",
    "    \"\"\"\n",
    "    A simplified DP-EBM training function that works on all columns.\n",
    "    \n",
    "    For numeric features the binning is done using quantiles;\n",
    "    for non-numeric (categorical) features each unique value is assigned its own bin.\n",
    "    \n",
    "    The Gaussian noise scale is computed using the Gaussian mechanism calibration\n",
    "    assuming sensitivity Δ = 1:\n",
    "    \n",
    "         noise_scale = (sqrt(2 * ln(1.25/delta))) / epsilon\n",
    "    \n",
    "    Parameters:\n",
    "      X            : DataFrame with features (numeric or categorical).\n",
    "      y            : Binary target as a 1D numpy array (0/1).\n",
    "      n_bins       : Maximum number of bins for numeric features.\n",
    "      n_epochs     : Number of boosting epochs (cycles over features).\n",
    "      learning_rate: Step size for updating shape functions.\n",
    "      epsilon      : Privacy budget.\n",
    "      delta        : Failure probability.\n",
    "      \n",
    "    Returns:\n",
    "      shape_functions: dict mapping feature names to learned arrays (one per bin).\n",
    "      bin_info       : dict mapping feature names to (bins, edges) information.\n",
    "      F              : Final additive prediction (before the sigmoid).\n",
    "    \"\"\"\n",
    "    # Compute noise scale from privacy parameters (assume sensitivity Δ = 1)\n",
    "    noise_scale = (np.sqrt(2 * np.log(1.25 / delta))) / epsilon\n",
    "\n",
    "    features = X.columns\n",
    "    bin_info = {}\n",
    "    shape_functions = {}\n",
    "    \n",
    "    # Precompute bin assignments and initialize shape functions for every feature.\n",
    "    for feature in features:\n",
    "        x = X[feature].values\n",
    "        bins, edges = bin_feature(x, n_bins=n_bins)\n",
    "        bin_info[feature] = (bins, edges)\n",
    "        # For numeric features, number of bins = len(edges) - 1; for categorical, = len(uniques)\n",
    "        if np.issubdtype(np.array(edges).dtype, np.number):\n",
    "            shape_functions[feature] = np.zeros(len(edges) - 1)\n",
    "        else:\n",
    "            shape_functions[feature] = np.zeros(len(edges))\n",
    "    \n",
    "    n = len(y)\n",
    "    F = np.zeros(n)\n",
    "    \n",
    "    # Boosting loop: cycle through all features and update their shape functions.\n",
    "    for epoch in range(n_epochs):\n",
    "        for feature in features:\n",
    "            x = X[feature].values\n",
    "            # For numeric features, use np.digitize; for categorical, we use a mapping based on sorted edges.\n",
    "            edges = bin_info[feature][1]\n",
    "            if np.issubdtype(np.array(edges).dtype, np.number):\n",
    "                x_numeric = x.astype(float)\n",
    "                bins = np.digitize(x_numeric, edges, right=False) - 1\n",
    "                bins = np.clip(bins, 0, len(shape_functions[feature]) - 1)\n",
    "            else:\n",
    "                mapping = {val: i for i, val in enumerate(edges)}\n",
    "                bins = np.array([mapping[val] for val in x])\n",
    "            \n",
    "            f = shape_functions[feature]\n",
    "            unique_bins = np.unique(bins)\n",
    "            for b in unique_bins:\n",
    "                idx = np.where(bins == b)[0]\n",
    "                if len(idx) == 0:\n",
    "                    continue\n",
    "                residuals = y[idx] - F[idx]\n",
    "                r_mean = np.mean(residuals)\n",
    "                noisy_update = learning_rate * (r_mean + dp_gaussian_noise(noise_scale))\n",
    "                f[b] += noisy_update\n",
    "                F[idx] += noisy_update\n",
    "            shape_functions[feature] = f\n",
    "    return shape_functions, bin_info, F\n",
    "\n",
    "# DP-EBM prediction\n",
    "def dp_ebm_predict(X, shape_functions, bin_info):\n",
    "    \"\"\"\n",
    "    Predict using the learned shape functions.\n",
    "    \n",
    "    For numeric features, bin using np.digitize;\n",
    "    for categorical features, use the mapping defined by the sorted unique values.\n",
    "    \n",
    "    Returns the additive prediction F (before applying the sigmoid).\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    F = np.zeros(n)\n",
    "    for feature in X.columns:\n",
    "        x = X[feature].values\n",
    "        edges = bin_info[feature][1]\n",
    "        if np.issubdtype(np.array(edges).dtype, np.number):\n",
    "            x_numeric = x.astype(float)\n",
    "            bin_indices = np.digitize(x_numeric, edges, right=False) - 1\n",
    "            bin_indices = np.clip(bin_indices, 0, len(shape_functions[feature]) - 1)\n",
    "        else:\n",
    "            mapping = {val: i for i, val in enumerate(edges)}\n",
    "            bin_indices = np.array([mapping.get(val, 0) for val in x])\n",
    "        F += shape_functions[feature][bin_indices]\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adult Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset Training AUC: 0.8417610798846461\n",
      "Adult Dataset Test AUC: 0.843542959565846\n"
     ]
    }
   ],
   "source": [
    "# Load Adult data\n",
    "df = load_adult_data()\n",
    "\n",
    "X = df.drop(columns=[\"Income\"])\n",
    "y = df[\"Income\"].values.astype(float)\n",
    "\n",
    "# Split data into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set privacy parameters.\n",
    "epsilon = 4.0   # privacy budget\n",
    "delta = 1e-5    # failure probability\n",
    "\n",
    "# Train DP-EBM on all columns.\n",
    "shape_functions, bin_info, F_train = dp_ebm_train(X_train, y_train, n_bins=32, n_epochs=280, learning_rate=0.01, epsilon=epsilon, delta=delta)\n",
    "\n",
    "# Evaluate on training data.\n",
    "train_preds_raw = F_train\n",
    "train_probs = sigmoid(train_preds_raw)\n",
    "train_preds = (train_probs > 0.5).astype(int)\n",
    "\n",
    "# print(\"Training Accuracy:\", accuracy_score(y_train, train_preds))\n",
    "print(\"Adult Dataset Training AUC:\", roc_auc_score(y_train, train_probs))\n",
    "\n",
    "# Predict on test data.\n",
    "F_test = dp_ebm_predict(X_test, shape_functions, bin_info)\n",
    "test_probs = sigmoid(F_test)\n",
    "test_preds = (test_probs > 0.5).astype(int)\n",
    "\n",
    "# print(\"Test Accuracy:\", accuracy_score(y_test, test_preds))\n",
    "print(\"Adult Dataset Test AUC:\", roc_auc_score(y_test, test_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telco Churn Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telco Churn Training AUC: 0.9515702256121044\n",
      "Telco Churn Test AUC: 0.8225063918763651\n"
     ]
    }
   ],
   "source": [
    "# Load Telco Churn data\n",
    "dataset = load_telco_churn_data()\n",
    "X = dataset['full']['X']\n",
    "y = dataset['full']['y'].values.astype(float)\n",
    "\n",
    "# Split data into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set privacy parameters.\n",
    "epsilon = 4.0   # privacy budget\n",
    "delta = 1e-5    # failure probability\n",
    "\n",
    "# Train the DP-EBM model.\n",
    "shape_functions, bin_info, F_train = dp_ebm_train(X_train, y_train, n_bins=32, n_epochs=125, learning_rate=0.001, epsilon=epsilon, delta=delta)\n",
    "\n",
    "# Evaluate training performance.\n",
    "train_preds_raw = F_train\n",
    "train_probs = sigmoid(train_preds_raw)\n",
    "train_auc = roc_auc_score(y_train, train_probs)\n",
    "print(\"Telco Churn Training AUC:\", train_auc)\n",
    "\n",
    "# Predict on the test set.\n",
    "F_test = dp_ebm_predict(X_test, shape_functions, bin_info)\n",
    "test_probs = sigmoid(F_test)\n",
    "test_auc = roc_auc_score(y_test, test_probs)\n",
    "print(\"Telco Churn Test AUC:\", test_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit Card Fraud Training AUC: 0.8583815352699271\n",
      "Credit Card Fraud Test AUC: 0.8754423371768516\n"
     ]
    }
   ],
   "source": [
    "# Load Credit Card Fraud data\n",
    "dataset = load_credit_data()\n",
    "X = dataset['full']['X']\n",
    "y = dataset['full']['y'].values.astype(float)\n",
    "\n",
    "# Split data into train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Set privacy parameters.\n",
    "epsilon = 4.0   # privacy budget\n",
    "delta = 1e-5    # failure probability\n",
    "\n",
    "# Train the DP-EBM model.\n",
    "shape_functions, bin_info, F_train = dp_ebm_train(X_train, y_train, n_bins=32, n_epochs=500, learning_rate=0.01, epsilon=epsilon, delta=delta)\n",
    "\n",
    "# Evaluate training performance.\n",
    "train_preds_raw = F_train\n",
    "train_probs = sigmoid(train_preds_raw)\n",
    "train_auc = roc_auc_score(y_train, train_probs)\n",
    "print(\"Credit Card Fraud Training AUC:\", train_auc)\n",
    "\n",
    "# Predict on the test set.\n",
    "F_test = dp_ebm_predict(X_test, shape_functions, bin_info)\n",
    "test_probs = sigmoid(F_test)\n",
    "test_auc = roc_auc_score(y_test, test_probs)\n",
    "print(\"Credit Card Fraud Test AUC:\", test_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
